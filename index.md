---
layout: project_page
permalink: /

title: "Multimodal Diffusion Transformer: Learning Versatile Behavior from Multimodal Goals"
authors: <a href="https://mbreuss.github.io/">Moritz Reuss</a>, Ömer Erdinç Yağmurlu, Fabian Wenzel, <a href="https://rudolf.intuitive-robots.net/">Rudolf Lioutikov</a>
affiliations: <a href="https://www.irl.iar.kit.edu/">Intuitive Robots Lab (IRL)</a></br>Karlsruhe Institute of Technology
paper: https://openreview.net/pdf?id=Pt6fLfXMRW
video: ./static/videos/mdt-v5-encoded.mp4
code: https://github.com/intuitive-robots/mdt_policy
# data: https://huggingface.co/docs/datasets
---

<!-- Using HTML to center the abstract -->
<video width="100%" autoplay controls muted loop playsinline>
    <source src="./static/videos/mdt-v5-encoded.mp4" type="video/mp4">
</video>

---

<div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
        <h2>Abstract</h2>
        <div class="content has-text-justified">
This work introduces the Multimodal Diffusion
Transformer (MDT), a novel diffusion policy framework, that
excels at learning versatile behavior from multimodal goal specifi-
cations with few language annotations. MDT leverages a diffusion
based multimodal transformer backbone and two self-supervised
auxiliary objectives to master long-horizon manipulation tasks
based on multimodal goals. The vast majority of imitation
learning methods only learn from individual goal modalities,
e.g. either language or goal images. However, existing large-
scale imitation learning datasets are only partially labeled with
language annotations, which prohibits current methods from
learning language conditioned behavior from these datasets.
MDT addresses this challenge by introducing a latent goal-
conditioned state representation, that is simultaneously trained
on multimodal goal instructions. This state representation aligns
image and language based goal embeddings and encodes suffi-
cient information to predict future states. The representation is
trained via two self-supervised auxiliary objectives that enhance
the performance of the presented transformer backbone. MDT
shows exceptional performance on 164 tasks provided by the chal-
lenging CALVIN and LIBERO benchmarks, including a LIBERO
version that contains less than 2% language annotations. Further,
MDT establishes a new record on the CALVIN manipulation
challenge, demonstrating an absolute performance improvement
of 15% over prior state-of-the-art methods, that require large-
scale pretraining and contain 10× more learnable parameters.
MDT demonstrated its ability to solve long-horizon manipulation
from sparsely annotated data in both simulated and real-world
environments.
        </div>
    </div>
</div>

---

<!-- > Note: This is an example of a Jekyll-based project website template: [Github link](https://github.com/shunzh/project_website).\
> The following content is generated by ChatGPT. The figure is manually added. -->

## Model Architecture
![MDT-V Overview](./static/image/mdt-v-figure.png)
**Left**: Overview of the proposed multimodal Transformer-Encoder-Decoder Diffusion Policy used in MDT.
**Right**: Specialized Diffusion Transformer Block for the Denoising of the Action Sequence.

MDT learns a goal-conditioned latent
state representation from multiple image observations and multimodal goals. The camera images are either processed with
frozen Voltron Encoders and a Perceiver or using ResNets. The separate GPT denoising module iteratively denoises an action
sequence of 10 steps with a Transformer Decoder with causal Attention. It consists of several Denoising Blocks, as visualized
on the right side. These blocks process noisy action tokens with self-attention and fuse the conditioning information from the
latent state representation via cross-attention. MDT applies adaLN conditioning to condition the blocks on the current
noise level. In addition, it aligns the latent representation tokens of the same state with different goal specifications using
self-supervised contrastive learning. The latent representation tokens are also used as a context input for the masked Image
Decoder module to reconstruct masked-out patches from future images.

### Masked Generative Foresight
<div class="column is-half is-pulled-right p-0">
    <img src="./static/image/mgf.png" alt="Masked Generative Foresight"/>
</div>
The Masked Generative Foresight Auxiliary Task
enhances the MDT model. It starts by encoding the current
observation and goal using the MDT Encoder. The resulting
latent state representations then serve as conditional inputs
for the Future Image-Decoder. This decoder receives encoded
patches of future camera images along with mask tokens. Its
task is to reconstruct the occluded patches in future frames.


### Contrastive Latent Alignment

Contrastive Latent Alignment auxiliary objective aligns
the MDT(-V) embeddings across
different goal modalities. These embeddings include the goal
as well as the current state information, allowing the CLA
objective to consider the task dynamics. 
Every training sample that is paired with a multimodal
goals specification is projected to latent vectors
for images and language goals respectively.
Contrastive Latent Alignment is achieved by using the InfoNCE loss with cosine similarity between the image and language projection.


## State-of-the-art on CALVIN ABCD→D
MDT-V sets a
new record in the CALVIN challenge, extending the average
rollout length to **4.51** which is a **10% absolute improvement**
over RoboFlamingo. MDT also surpasses all other tested
methods. Notably, MDT achieves this while having less than
10% of trainable parameters and not requiring pretraining
on large-scale datasets.

<div class="columns is-centered">
    <div class="column is-two-thirds">
        <img src="./static/image/calvin-abcd.png" alt="CALVIN ABCD->D"/>
    </div>
</div>
<div class="columns is-mobile is-multiline is-centered">
    <div class="column is-half-mobile is-one-third-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/mdt_02.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-half-mobile is-one-third-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/5_seq_mdt_rollout_text_3.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-half-mobile is-one-third-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/5-seq_mdt_rollout_text_4.mp4" type="video/mp4">
        </video>
    </div>
</div>
<div class="columns is-mobile is-multiline is-centered">
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p0_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p1_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p2_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p3_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p4_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p5_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p6_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p7_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p8_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p9_videos/video.mp4" type="video/mp4">
        </video>
    </div>
</div>

## LIBERO with less than 2% Language Annotations

In the LIBERO task suites, MDT proves to be effective with sparsely labeled
data, outperforming the Oracle-BC baseline, which relies on
fully labeled demonstrations. MDT not only outperforms the
fully language-labeled Transformer Baseline in three out of
four challenges but also significantly surpasses the U-Net-
based Distill-D policy in all tests by a wide margin, even
without auxiliary objectives. The performance of MDT on the
LIBERO-90 suite demonstrates that both objectives and our
policy learn best from a large dataset. The proposed auxiliary
objectives further improve the average performance of MDT
by 8.5% averaged over all 5 task suites.

<div class="columns is-mobile is-multiline is-centered">
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p0_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p1_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p2_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p3_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p4_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p5_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p6_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p7_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p8_videos/video.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-one-third-mobile is-one-fifth-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/libero_10_22_onk9_p9_videos/video.mp4" type="video/mp4">
        </video>
    </div>
</div>

## Real Robot Experiments
Real world play dataset encompasses
around 4.5 hours of interactive **play data** with **20 different
tasks** for the policies to learn. Play demonstrations last
from around 30 seconds to more than 450 seconds and contain
between 5 and 20 tasks. The dataset is **partially labeled** by
randomly identifying some tasks in the demonstrations and
annotating the respective interval, yielding a total of 360 labels (~18 labels per task)
or approximately 20% of the dataset.

### Sample Demonstration from the Real Robot Dataset
<video width="100%" autoplay controls muted loop playsinline>
    <source src="./static/videos/demonstration.mp4" type="video/mp4">
</video>
### Evaluation Videos
#### Multi-task
<div class="columns is-centered is-mobile">
    <div class="column is-half">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/m1.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-half">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/m2.mp4" type="video/mp4">
        </video>
    </div>    
</div>
#### Single-task
<div class="columns is-centered is-multiline is-mobile">
    <div class="column is-half-mobile is-one-quarter-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/s1.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-half-mobile is-one-quarter-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/s2.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-half-mobile is-one-quarter-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/s3.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-half-mobile is-one-quarter-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/s4.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-half-mobile is-one-quarter-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/s5.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-half-mobile is-one-quarter-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/s6.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-half-mobile is-one-quarter-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/s7.mp4" type="video/mp4">
        </video>
    </div>
    <div class="column is-half-mobile is-one-quarter-tablet">
        <video width="100%" autoplay controls muted loop playsinline>
            <source src="./static/videos/s8.mp4" type="video/mp4">
        </video>
    </div>
</div>

## Citation
```
@inproceedings{
    reuss2024multimodal,
    title={Multimodal Diffusion Transformer: Learning Versatile Behavior from Multimodal Goals},
    author={Moritz Reuss and {\"O}mer Erdin{\c{c}} Ya{\u{g}}murlu and Fabian Wenzel and Rudolf Lioutikov},
    booktitle={Robotics: Science and Systems},
    year={2024}
    }
```
